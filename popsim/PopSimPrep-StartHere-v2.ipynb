{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PopSim Data Preparation (Census Mode)\n\nThis notebook automates population synthesis setup using German Census grid data.\n\n## What You Need\n\n1. **A GeoPackage** (`.gpkg`) defining your study area boundary\n2. **Census parquet files** in the `data/` folder:\n   - `cells_100m_*.parquet` - 100m grid data\n   - `cells_1km_*.parquet` - 1km grid data\n\n## Census Data Format\n\nAny parquet file works as \"census\" data as long as the **first column** contains cell IDs in this format:\n- **100m**: `CRS3035RES100mN{northing}E{easting}` (e.g., `CRS3035RES100mN2689100E4337000`)\n- **1km**: `CRS3035RES1000mN{northing}E{easting}` (e.g., `CRS3035RES1000mN2689000E4337000`)\n\nThe coordinates are in **EPSG:3035** (ETRS89-extended / LAEA Europe). All other columns become available as control totals.\n\n## What Gets Automated\n\n- Census cells are automatically filtered to your study area (via GeoPackage)\n- Geographic crosswalk (100m → 1km → STAAT) is auto-generated\n- Control totals are auto-populated from census columns\n- You only need to define control expressions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\n**Edit these settings before running.**\n\n### CSV Separator Notes\n- `census_csv_sep`: Separator for **input** CSV files (ignored for parquet). Use `;` for German/European data.\n- `intermediate_sep`: Separator for intermediate files (`_prep3_controls.csv`). Use `;` if editing in Excel with German locale.\n- **Final files** (consumed by PopSim) are **always comma-separated** - this is required for PopSim compatibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# USER CONFIGURATION\n# =============================================================================\n\n# Path to your study area GeoPackage\ngeopackage_path = \"data/outlineNIwithBremen.gpkg\"\n\n# GeoPackage CRS (set to None to auto-detect, or specify e.g., \"EPSG:25832\")\ngeopackage_crs = \"EPSG:25832\"  # UTM 32N - common for German data\n\n# Census data files (parquet or CSV)\ncensus_100m_path = \"data/cells_100m_with_gender_backf_binneds_happyorphans.parquet\"\ncensus_1km_path = \"data/cells_1km_with_binneds.parquet\"\n\n# CSV settings (ignored for parquet files)\ncensus_csv_sep = \";\"  # Separator for input CSV files: \",\" or \";\"\n\n# Intermediate file separator (for _prep*, _census_filtered, etc.)\n# Use \";\" if your locale uses comma as decimal separator\nintermediate_sep = \",\"  # \",\" or \";\" - finals are always \",\"\n\n# Column to use as number of households (from census data)\n# Set to None to see available columns, then set and re-run\nhousehold_column = None  # e.g., \"Insgesamt_Haushalte_100m-Gitter\"\n\n# Output settings\noutput_everything = False  # Set True to output all intermediate PopSim files\n\n# Geography settings\nseed_geography = \"STAAT\"  # Usually unchanged\n\n# MiD (seed data) filtering\nfilter_mid = False\nkernwo = [2, 3]  # Day of week filter (list): 1=Monday, 2=Tue-Thu, 3=Friday, 4=Sat-Sun\nregiostar17 = [121, 123, 124]  # Regional types to include\n\n# =============================================================================\n# END CONFIGURATION\n# ============================================================================="
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Study Area and Filter Census\n",
    "\n",
    "Loads your GeoPackage, filters census cells to the study area, and shows available columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nimport re\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import box\n\nprint(\"[Step 1/4] Loading study area and filtering census...\")\nprint(\"=\" * 60)\n\n# Helper to load census file (parquet or CSV)\ndef load_census_file(path, csv_sep=\",\", batch_mode=False):\n    \"\"\"Load census data from parquet or CSV.\"\"\"\n    if path.endswith('.parquet'):\n        if batch_mode:\n            import pyarrow.parquet as pq\n            return pq.ParquetFile(path)\n        return pd.read_parquet(path)\n    elif path.endswith('.csv'):\n        return pd.read_csv(path, sep=csv_sep)\n    else:\n        raise ValueError(f\"Unsupported file format: {path}. Use .parquet or .csv\")\n\n# Load GeoPackage\nprint(f\"Loading GeoPackage: {geopackage_path}\")\nstudy_area = gpd.read_file(geopackage_path)\n\n# Handle CRS\nif study_area.crs is None and geopackage_crs:\n    study_area = study_area.set_crs(geopackage_crs)\n    print(f\"  Set CRS to: {geopackage_crs}\")\nelif study_area.crs is None:\n    raise ValueError(\"GeoPackage has no CRS. Please set geopackage_crs in configuration.\")\n\n# Transform to EPSG:3035 (Census CRS)\nstudy_area_3035 = study_area.to_crs(\"EPSG:3035\")\nbounds = study_area_3035.total_bounds  # minx, miny, maxx, maxy\nprint(f\"  Study area bounds (EPSG:3035): {bounds}\")\n\n# Parse cell ID to extract coordinates\ndef parse_cell_id_100m(cell_id):\n    \"\"\"Extract N,E coordinates from 100m cell ID like CRS3035RES100mN2689100E4337000\"\"\"\n    match = re.match(r'CRS3035RES100mN(\\d+)E(\\d+)', str(cell_id))\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None, None\n\ndef parse_cell_id_1km(cell_id):\n    \"\"\"Extract N,E coordinates from 1km cell ID like CRS3035RES1000mN2689000E4337000\"\"\"\n    match = re.match(r'CRS3035RES1000mN(\\d+)E(\\d+)', str(cell_id))\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None, None\n\ndef get_1km_id_from_100m(cell_id_100m):\n    \"\"\"Convert 100m cell ID to corresponding 1km cell ID.\"\"\"\n    n, e = parse_cell_id_100m(cell_id_100m)\n    if n is None:\n        return None\n    # Round down to nearest 1000m\n    n_1km = (n // 1000) * 1000\n    e_1km = (e // 1000) * 1000\n    return f\"CRS3035RES1000mN{n_1km}E{e_1km}\"\n\n# Load 100m census\nprint(f\"\\nLoading 100m census: {census_100m_path}\")\n\nif census_100m_path.endswith('.parquet'):\n    import pyarrow.parquet as pq\n    pf_100m = pq.ParquetFile(census_100m_path)\n    print(f\"  Total rows: {pf_100m.metadata.num_rows:,}\")\n    print(f\"  Total columns: {pf_100m.metadata.num_columns}\")\n    \n    # Read in batches to filter efficiently\n    print(\"  Filtering to study area (this may take a moment)...\")\n    filtered_chunks = []\n    total_read = 0\n    \n    for batch in pf_100m.iter_batches(batch_size=100000):\n        df_batch = batch.to_pandas()\n        total_read += len(df_batch)\n        \n        # Parse coordinates and filter\n        coords = df_batch.iloc[:, 0].apply(parse_cell_id_100m)\n        df_batch['_N'] = coords.apply(lambda x: x[0])\n        df_batch['_E'] = coords.apply(lambda x: x[1])\n        \n        # Bounding box filter\n        mask = (\n            (df_batch['_N'] >= bounds[1]) & (df_batch['_N'] <= bounds[3]) &\n            (df_batch['_E'] >= bounds[0]) & (df_batch['_E'] <= bounds[2])\n        )\n        df_filtered = df_batch[mask].drop(columns=['_N', '_E'])\n        \n        if len(df_filtered) > 0:\n            filtered_chunks.append(df_filtered)\n        \n        if total_read % 500000 == 0:\n            print(f\"    Processed {total_read:,} rows...\")\n    \n    census_100m = pd.concat(filtered_chunks, ignore_index=True)\nelse:\n    # CSV - load all at once (usually smaller files)\n    print(f\"  Loading CSV with separator: '{census_csv_sep}'\")\n    census_100m_full = pd.read_csv(census_100m_path, sep=census_csv_sep)\n    print(f\"  Total rows: {len(census_100m_full):,}\")\n    \n    # Filter by bounding box\n    coords = census_100m_full.iloc[:, 0].apply(parse_cell_id_100m)\n    census_100m_full['_N'] = coords.apply(lambda x: x[0])\n    census_100m_full['_E'] = coords.apply(lambda x: x[1])\n    \n    mask = (\n        (census_100m_full['_N'] >= bounds[1]) & (census_100m_full['_N'] <= bounds[3]) &\n        (census_100m_full['_E'] >= bounds[0]) & (census_100m_full['_E'] <= bounds[2])\n    )\n    census_100m = census_100m_full[mask].drop(columns=['_N', '_E']).copy()\n\nprint(f\"  Filtered to {len(census_100m):,} cells in bounding box\")\n\n# Fine filter: check actual intersection with study area polygon\nprint(\"  Performing precise polygon intersection...\")\nid_col = census_100m.columns[0]\n\ndef cell_intersects_study_area(cell_id):\n    n, e = parse_cell_id_100m(cell_id)\n    if n is None:\n        return False\n    cell_geom = box(e, n, e + 100, n + 100)\n    return study_area_3035.geometry.intersects(cell_geom).any()\n\n# Sample check - if bbox is tight, skip full intersection\nsample_mask = census_100m[id_col].sample(min(100, len(census_100m))).apply(cell_intersects_study_area)\nif sample_mask.mean() > 0.9:\n    print(\"  Bounding box is tight, skipping detailed intersection.\")\nelse:\n    mask = census_100m[id_col].apply(cell_intersects_study_area)\n    census_100m = census_100m[mask]\n    print(f\"  After polygon intersection: {len(census_100m):,} cells\")\n\n# Show available columns for household selection\nprint(f\"\\n{'='*60}\")\nprint(\"AVAILABLE COLUMNS IN 100m CENSUS DATA:\")\nprint(f\"{'='*60}\")\n\n# Find likely household columns\nhh_keywords = ['haushalt', 'household', 'hh_', 'wohnung']\nsuggested = []\nfor col in census_100m.columns:\n    col_lower = col.lower()\n    if any(kw in col_lower for kw in hh_keywords):\n        suggested.append(col)\n\nif suggested:\n    print(\"\\nSUGGESTED HOUSEHOLD COLUMNS:\")\n    for col in suggested[:10]:\n        print(f\"  -> {col}\")\n\nprint(f\"\\nALL COLUMNS ({len(census_100m.columns)}):\")\nfor i, col in enumerate(census_100m.columns):\n    if i < 30:\n        print(f\"  {col}\")\n    elif i == 30:\n        print(f\"  ... and {len(census_100m.columns) - 30} more\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA PREVIEW (first 3 rows, first 5 columns):\")\nprint(f\"{'='*60}\")\nprint(census_100m.iloc[:3, :5].to_string())\n\n# Load 1km census\nprint(f\"\\n{'='*60}\")\nprint(f\"Loading 1km census: {census_1km_path}\")\n\nif census_1km_path.endswith('.parquet'):\n    census_1km_full = pd.read_parquet(census_1km_path)\nelse:\n    census_1km_full = pd.read_csv(census_1km_path, sep=census_csv_sep)\nprint(f\"  Total rows: {len(census_1km_full):,}\")\n\n# Filter 1km by deriving from 100m cells\nkm_ids_needed = set(census_100m[id_col].apply(get_1km_id_from_100m).dropna())\nkm_id_col = census_1km_full.columns[1]  # Usually GITTER_ID_1km\ncensus_1km = census_1km_full[census_1km_full[km_id_col].isin(km_ids_needed)].copy()\nprint(f\"  Filtered to {len(census_1km):,} 1km cells\")\n\n# Save filtered data as parquet (always - most efficient for intermediates)\ncensus_100m.to_parquet('data/_census_100m_filtered.parquet', index=False)\ncensus_1km.to_parquet('data/_census_1km_filtered.parquet', index=False)\nprint(f\"\\nSaved filtered census to data/_census_*_filtered.parquet\")\n\n# Normalize kernwo to list\nkernwo_list = kernwo if isinstance(kernwo, list) else [kernwo]\n\n# Save config\nconfig = {\n    \"geopackage_path\": geopackage_path,\n    \"census_100m_id_col\": id_col,\n    \"census_1km_id_col\": km_id_col,\n    \"household_column\": household_column,\n    \"output_everything\": output_everything,\n    \"seed_geography\": seed_geography,\n    \"filter_mid\": filter_mid,\n    \"kernwo\": kernwo_list,\n    \"regiostar17\": regiostar17,\n    \"intermediate_sep\": intermediate_sep,\n    \"num_100m_cells\": len(census_100m),\n    \"num_1km_cells\": len(census_1km),\n}\nwith open(\"prep_config.json\", \"w\") as f:\n    json.dump(config, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(\"SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"  100m cells in study area: {len(census_100m):,}\")\nprint(f\"  1km cells in study area: {len(census_1km):,}\")\nprint(f\"  Intermediate file separator: '{intermediate_sep}'\")\nprint(f\"\\nSet 'household_column' in Configuration cell and re-run Step 1,\")\nprint(\"or proceed to Step 2 if already set.\")\nprint(\"\\n[Step 1/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Geo Crosswalk and Control Totals\n",
    "\n",
    "Automatically creates:\n",
    "- `geo_cross_walk.csv` - mapping 100m → 1km → STAAT\n",
    "- `control_totals_*.csv` - census data as control totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport re\nimport yaml\nfrom unidecode import unidecode\n\nprint(\"[Step 2/4] Generating geo crosswalk and control totals...\")\nprint(\"=\" * 60)\n\n# Load config\nwith open(\"prep_config.json\", \"r\") as f:\n    config = json.load(f)\n\nhousehold_column = config[\"household_column\"]\nid_col_100m = config[\"census_100m_id_col\"]\nid_col_1km = config[\"census_1km_id_col\"]\noutput_everything = config[\"output_everything\"]\nseed_geography = config[\"seed_geography\"]\nintermediate_sep = config.get(\"intermediate_sep\", \",\")  # Default to comma\n\nif household_column is None:\n    raise ValueError(\"household_column not set! Set it in Configuration and re-run Step 1.\")\n\n# Load filtered census\ncensus_100m = pd.read_parquet('data/_census_100m_filtered.parquet')\ncensus_1km = pd.read_parquet('data/_census_1km_filtered.parquet')\n\nprint(f\"Loaded {len(census_100m):,} 100m cells, {len(census_1km):,} 1km cells\")\n\n# Validate household column\nif household_column not in census_100m.columns:\n    raise ValueError(f\"household_column '{household_column}' not found in census data.\")\n\n# Check household values\nhh_values = census_100m[household_column]\nif hh_values.isna().any():\n    na_count = hh_values.isna().sum()\n    print(f\"WARNING: {na_count} cells have missing household values (will be set to 0)\")\nif (hh_values < 0).any():\n    neg_count = (hh_values < 0).sum()\n    print(f\"WARNING: {neg_count} cells have negative household values\")\n\n# Helper to get 1km ID from 100m ID\ndef get_1km_from_100m(cell_id):\n    \"\"\"Convert 100m cell ID to corresponding 1km cell ID.\"\"\"\n    match = re.match(r'CRS3035RES100mN(\\d+)E(\\d+)', str(cell_id))\n    if match:\n        n, e = int(match.group(1)), int(match.group(2))\n        # Round down to nearest 1000m\n        n_1km = (n // 1000) * 1000\n        e_1km = (e // 1000) * 1000\n        return f\"CRS3035RES1000mN{n_1km}E{e_1km}\"\n    return None\n\n# Standardize column names\ndef clean_col_name(name):\n    return unidecode(name).replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"-\", \"_\")\n\n# Rename columns\ncensus_100m.columns = [clean_col_name(c) for c in census_100m.columns]\ncensus_1km.columns = [clean_col_name(c) for c in census_1km.columns]\nhousehold_column_clean = clean_col_name(household_column)\n\n# Find the ID columns after cleaning\nid_col_100m_clean = census_100m.columns[0]\nid_col_1km_clean = [c for c in census_1km.columns if '1km' in c.lower()][0]\n\n# Create geo_cross_walk (hierarchy: ZENSUS100m -> ZENSUS1km -> STAAT -> WELT)\nprint(\"\\nCreating geo_cross_walk...\")\ngeo_cross = pd.DataFrame()\ngeo_cross['ZENSUS100m'] = census_100m[id_col_100m_clean]\ngeo_cross['ZENSUS1km'] = geo_cross['ZENSUS100m'].apply(get_1km_from_100m)\ngeo_cross['STAAT'] = 1\ngeo_cross['WELT'] = 1\n\ngeo_cross.to_csv('data/geo_cross_walk.csv', index=False)  # Final - always comma\nprint(f\"  Created data/geo_cross_walk.csv ({len(geo_cross)} rows)\")\n\n# Create control_totals for 100m (lowest level)\nprint(\"\\nCreating control totals...\")\n\n# Geography names (hierarchy from lowest to highest)\ngeo_names = ['ZENSUS100m', 'ZENSUS1km', 'STAAT', 'WELT']\n\n# Rename household column to numberOfHouseholds\ncensus_100m = census_100m.rename(columns={household_column_clean: 'numberOfHouseholds'})\n\n# Add geography columns\ncensus_100m = census_100m.rename(columns={id_col_100m_clean: 'ZENSUS100m'})\ncensus_100m['ZENSUS1km'] = census_100m['ZENSUS100m'].apply(get_1km_from_100m)\ncensus_100m['STAAT'] = 1\ncensus_100m['WELT'] = 1\n\n# Suffix non-geo columns\nfor col in census_100m.columns:\n    if col not in geo_names:\n        census_100m.rename(columns={col: f\"{col}_ZENSUS100m\"}, inplace=True)\n\ncensus_100m = census_100m.fillna(0)\ncensus_100m.to_csv('data/control_totals_ZENSUS100m.csv', index=False)  # Final - always comma\nprint(f\"  Created data/control_totals_ZENSUS100m.csv\")\n\n# Create control_totals for 1km\ncensus_1km = census_1km.rename(columns={id_col_1km_clean: 'ZENSUS1km'})\ncensus_1km['STAAT'] = 1\ncensus_1km['WELT'] = 1\n\nfor col in census_1km.columns:\n    if col not in geo_names:\n        census_1km.rename(columns={col: f\"{col}_ZENSUS1km\"}, inplace=True)\n\ncensus_1km = census_1km.fillna(0)\ncensus_1km.to_csv('data/control_totals_ZENSUS1km.csv', index=False)  # Final - always comma\nprint(f\"  Created data/control_totals_ZENSUS1km.csv\")\n\n# Create control_totals for STAAT\nstaat_df = pd.DataFrame({'STAAT': [1], 'WELT': [1]})\nstaat_df.to_csv('data/control_totals_STAAT.csv', index=False)  # Final - always comma\nprint(f\"  Created data/control_totals_STAAT.csv\")\n\n# Create control_totals for WELT (top level)\nwelt_df = pd.DataFrame({'WELT': [1]})\nwelt_df.to_csv('data/control_totals_WELT.csv', index=False)  # Final - always comma\nprint(f\"  Created data/control_totals_WELT.csv\")\n\n# Create controls template\nprint(\"\\nCreating controls template...\")\ncontrols_data = {\n    'target': [],\n    'geography': [],\n    'seed_table': [],\n    'importance': [],\n    'control_field': [],\n    'expression': []\n}\n\ntotal_hh_control = None\n\n# Add 100m controls\nfor col in census_100m.columns:\n    if col not in geo_names:\n        controls_data['target'].append(f\"{col}_target\")\n        controls_data['geography'].append('ZENSUS100m')\n        controls_data['control_field'].append(col)\n        if col.startswith('numberOfHouseholds'):\n            total_hh_control = f\"{col}_target\"\n\n# Add 1km controls\nfor col in census_1km.columns:\n    if col not in geo_names:\n        controls_data['target'].append(f\"{col}_target\")\n        controls_data['geography'].append('ZENSUS1km')\n        controls_data['control_field'].append(col)\n\n# Add example expression\ncontrols_data['expression'].append('(households.H_GEW > 0) & (households.H_GEW < np.inf)')\n\ncontrols_df = pd.DataFrame({k: pd.Series(v) for k, v in controls_data.items()})\n# Intermediate file - use configured separator\ncontrols_df.to_csv('configs/_prep3_controls.csv', index=False, sep=intermediate_sep)\nprint(f\"  Created configs/_prep3_controls.csv ({len(controls_df)} controls, sep='{intermediate_sep}')\")\n\nif total_hh_control is None:\n    raise ValueError(\"Could not find numberOfHouseholds control!\")\n\n# Update settings.yaml\nprint(\"\\nUpdating PopSim configuration...\")\nwith open('configs/settings.yaml', 'r') as f:\n    settings = yaml.safe_load(f)\n\n# Geographies from top to bottom: WELT -> STAAT -> ZENSUS1km -> ZENSUS100m\nsettings['geographies'] = ['WELT', 'STAAT', 'ZENSUS1km', 'ZENSUS100m']\nsettings['seed_geography'] = seed_geography\nsettings['total_hh_control'] = total_hh_control\n\n# Update input tables\nidx = next((i for i, t in enumerate(settings['input_table_list']) if t['tablename'] == 'geo_cross_walk'), None)\nif idx is not None:\n    settings['input_table_list'] = settings['input_table_list'][:idx + 1]\n\nfor geo in ['ZENSUS100m', 'ZENSUS1km', 'STAAT', 'WELT']:\n    settings['input_table_list'].append({\n        'tablename': f'{geo}_control_data',\n        'filename': f'control_totals_{geo}.csv'\n    })\n\n# Update output tables\nif output_everything:\n    settings['output_tables'] = {'action': 'skip', 'tables': 'geo_cross_walk'}\nelse:\n    settings['output_tables'] = {\n        'action': 'include',\n        'tables': ['expanded_household_ids', \n                   'summary_ZENSUS100m', 'summary_ZENSUS1km', 'summary_STAAT', 'summary_WELT',\n                   f'summary_ZENSUS100m_{seed_geography}']\n    }\n\n# Update models\nsettings['models'] = [m for m in settings['models'] if 'sub_balancing' not in m]\nidx = settings['models'].index('integerize_final_seed_weights')\nsettings['models'].insert(idx + 1, 'sub_balancing.geography=ZENSUS100m')\n\nwith open('configs/settings.yaml', 'w') as f:\n    yaml.dump(settings, f, default_flow_style=False)\nprint(\"  Updated configs/settings.yaml\")\n\n# Update verification.yaml\nwith open('scripts/verification.yaml', 'r') as f:\n    verify = yaml.safe_load(f)\n\nverify['group_geographies'] = ['WELT', 'STAAT', 'ZENSUS1km', 'ZENSUS100m']\nverify['seed_cols']['geog'] = seed_geography\nverify['summaries'] = [\n    'output/final_summary_ZENSUS100m.csv',\n    'output/final_summary_ZENSUS1km.csv',\n    'output/final_summary_STAAT.csv',\n    'output/final_summary_WELT.csv',\n    f'output/final_summary_ZENSUS100m_{seed_geography}.csv'\n]\n\nwith open('scripts/verification.yaml', 'w') as f:\n    yaml.dump(verify, f, default_flow_style=False)\nprint(\"  Updated scripts/verification.yaml\")\n\n# Update config\nconfig['geo_names'] = geo_names\nconfig['total_hh_control'] = total_hh_control\nwith open('prep_config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\n# Summary\nprint(f\"\\n{'='*60}\")\nprint(\"SUMMARY\")\nprint(f\"{'='*60}\")\nhh_col = 'numberOfHouseholds_ZENSUS100m'\ntotal_hh = census_100m[hh_col].sum()\nprint(f\"  Geographic hierarchy: WELT -> STAAT -> ZENSUS1km -> ZENSUS100m\")\nprint(f\"  Geographic units: {len(census_100m):,} (100m), {len(census_1km):,} (1km)\")\nprint(f\"  Total households: {total_hh:,.0f}\")\nprint(f\"  Controls defined: {len(controls_df)}\")\nprint(f\"  Intermediate separator: '{intermediate_sep}'\")\nprint(f\"\\nNext: Edit configs/_prep3_controls.csv to add expressions for controls you want.\")\nprint(\"\\n[Step 2/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Edit Controls and Process Seed Data\n",
    "\n",
    "Edit `configs/_prep3_controls.csv` to add expressions for the controls you want to use, then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\nimport json\nimport re\nimport math\n\nprint(\"[Step 3/4] Processing seed data...\")\nprint(\"=\" * 60)\n\n# Load config\nwith open(\"prep_config.json\", \"r\") as f:\n    config = json.load(f)\n\nfilter_mid = config[\"filter_mid\"]\nkernwo = config[\"kernwo\"]  # Now a list\nregiostar17 = config[\"regiostar17\"]\nintermediate_sep = config.get(\"intermediate_sep\", \",\")  # Default to comma\n\n# Load controls (intermediate file - use configured separator)\nprint(f\"Loading controls template (separator: '{intermediate_sep}')...\")\ncontrols_df = pd.read_csv('configs/_prep3_controls.csv', sep=intermediate_sep)\nprint(f\"  Loaded {len(controls_df)} controls from _prep3_controls.csv\")\n\n# Load seed data (MiD files always use semicolon - German standard)\nprint(\"\\nLoading MiD seed data...\")\nseed_persons = pd.read_csv('data/MiD2017_Personen.csv', sep=';')\nseed_households = pd.read_csv('data/MiD2017_Haushalte.csv', sep=';')\n\nprint(f\"  Loaded {len(seed_persons):,} persons, {len(seed_households):,} households\")\n\nif filter_mid:\n    print(f\"\\nApplying MiD filters:\")\n    persons_before = len(seed_persons)\n    households_before = len(seed_households)\n    \n    # Filter by kernwo (day of week) - applies to persons\n    if 'kernwo' in seed_persons.columns:\n        seed_persons = seed_persons[seed_persons['kernwo'].isin(kernwo)]\n        print(f\"  kernwo filter {kernwo}: {persons_before:,} -> {len(seed_persons):,} persons\")\n    \n    # Filter by RegioStaR17\n    if 'RegioStaR17' in seed_persons.columns:\n        seed_persons = seed_persons[seed_persons['RegioStaR17'].isin(regiostar17)]\n    if 'RegioStaR17' in seed_households.columns:\n        seed_households = seed_households[seed_households['RegioStaR17'].isin(regiostar17)]\n    print(f\"  RegioStaR17 filter {regiostar17}: {len(seed_persons):,} persons, {len(seed_households):,} households\")\n\nprint(f\"\\nFinal counts:\")\nprint(f\"  Persons: {len(seed_persons):,}\")\nprint(f\"  Households: {len(seed_households):,}\")\n\n# Essential columns\nessential_cols = {'H_ID', 'H_GEW', 'HP_ID', 'P_ID', 'P_GEW'}\nneeded_cols = essential_cols.copy()\n\n# Extract columns from expressions\npattern = r'\\.(?P<col>[A-Za-z_][A-Za-z0-9_]*)'\nfor expr in controls_df['expression'].dropna():\n    for match in re.finditer(pattern, str(expr)):\n        needed_cols.add(match.group('col'))\n\nprint(f\"\\nColumns needed from expressions: {needed_cols - essential_cols}\")\n\n# Filter to needed columns\np_cols = list(needed_cols.intersection(seed_persons.columns))\nh_cols = list(needed_cols.intersection(seed_households.columns))\n\nseed_persons = seed_persons[p_cols]\nseed_households = seed_households[h_cols]\n\n# Add STAAT geography\nseed_persons['STAAT'] = 1\nseed_households['STAAT'] = 1\n\n# Save (final files - always comma separated for PopSim compatibility)\nseed_persons.to_csv('data/seed_persons.csv', index=False)  # Final - always comma\nseed_households.to_csv('data/seed_households.csv', index=False)  # Final - always comma\ncontrols_df.to_csv('configs/controls.csv', index=False)  # Final - always comma\n\nprint(f\"\\nCreated (all comma-separated for PopSim):\")\nprint(f\"  data/seed_persons.csv ({len(seed_persons)} rows, {len(seed_persons.columns)} cols)\")\nprint(f\"  data/seed_households.csv ({len(seed_households)} rows, {len(seed_households.columns)} cols)\")\nprint(f\"  configs/controls.csv\")\n\nprint(\"\\n[Step 3/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate and Run\n",
    "\n",
    "Validates the setup and provides instructions for running PopSim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport yaml\nimport pandas as pd\n\nprint(\"[Step 4/4] Validating setup...\")\nprint(\"=\" * 60)\n\nerrors = []\nwarnings = []\n\n# Check files\nrequired_files = [\n    'data/geo_cross_walk.csv',\n    'data/seed_persons.csv',\n    'data/seed_households.csv',\n    'data/control_totals_ZENSUS100m.csv',\n    'data/control_totals_ZENSUS1km.csv',\n    'data/control_totals_STAAT.csv',\n    'data/control_totals_WELT.csv',\n    'configs/settings.yaml',\n    'configs/controls.csv',\n]\n\nprint(\"\\nChecking files...\")\nfor f in required_files:\n    if os.path.exists(f):\n        size = os.path.getsize(f)\n        print(f\"  [OK] {f} ({size:,} bytes)\")\n    else:\n        print(f\"  [MISSING] {f}\")\n        errors.append(f\"Missing: {f}\")\n\n# Check controls\nprint(\"\\nChecking controls...\")\ntry:\n    controls = pd.read_csv('configs/controls.csv')\n    empty = controls['expression'].isna().sum()\n    if empty > 0:\n        errors.append(f\"{empty} controls missing expressions\")\n    else:\n        print(f\"  {len(controls)} controls, all have expressions\")\nexcept Exception as e:\n    errors.append(f\"Error reading controls: {e}\")\n\n# Check settings\nprint(\"\\nChecking settings.yaml...\")\ntry:\n    with open('configs/settings.yaml') as f:\n        settings = yaml.safe_load(f)\n    print(f\"  Geographies: {settings.get('geographies')}\")\n    print(f\"  Total HH control: {settings.get('total_hh_control')}\")\nexcept Exception as e:\n    errors.append(f\"Error reading settings: {e}\")\n\n# Summary\nprint(f\"\\n{'='*60}\")\nif errors:\n    print(\"VALIDATION FAILED\")\n    for e in errors:\n        print(f\"  - {e}\")\nelse:\n    print(\"VALIDATION PASSED\")\n    print(\"\\nReady to run PopSim:\")\n    print(\"  conda activate popsim\")\n    print(\"  python run_populationsim.py\")\n\nprint(f\"{'='*60}\")\nprint(\"\\n[Step 4/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities: Reset\n",
    "\n",
    "Clean up generated files to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\ndef reset(confirm=False):\n    \"\"\"Delete all generated files.\"\"\"\n    files = [\n        'data/geo_cross_walk.csv',\n        'data/seed_persons.csv',\n        'data/seed_households.csv',\n        'data/control_totals_ZENSUS100m.csv',\n        'data/control_totals_ZENSUS1km.csv',\n        'data/control_totals_STAAT.csv',\n        'data/control_totals_WELT.csv',\n        'data/_census_100m_filtered.parquet',\n        'data/_census_1km_filtered.parquet',\n        'configs/controls.csv',\n        'configs/_prep3_controls.csv',\n        'prep_config.json',\n    ]\n    \n    existing = [f for f in files if os.path.exists(f)]\n    \n    if not existing:\n        print(\"No files to delete.\")\n        return\n    \n    print(\"Files to delete:\")\n    for f in existing:\n        print(f\"  {f}\")\n    \n    if not confirm:\n        print(\"\\nRun reset(confirm=True) to delete.\")\n        return\n    \n    for f in existing:\n        os.remove(f)\n        print(f\"Deleted: {f}\")\n    print(\"\\nReset complete.\")\n\n# Show what would be deleted\nreset(confirm=False)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}