{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PopSim Data Preparation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "A template named **\"_prep0_geo_cross_walk.csv\"** is already available in **\"data\"**. This is where the geocrosswalk data, e.g., from QGIS, should be inserted. The column names, meaning the names of the individual levels, can be freely chosen now but **cannot be renamed later**. An unlimited number of levels is possible. The example data can be deleted.\n",
    "\n",
    "If you want to use census levels, use \"ZENSUS100m\" and enter all cell keys in the study area. Place \"ZENSUS1km\" and \"ZENSUS10km\" headers at the correct level (left to right), their cell keys will be added automatically.\n",
    "\n",
    "From this, a template for the controls of the lowest level will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**All settings are centralized here.** Edit these values before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USER CONFIGURATION - Edit these values\n",
    "# =============================================================================\n",
    "\n",
    "# Output settings\n",
    "output_everything = False  # Set to True to output all intermediate PopSim files\n",
    "\n",
    "# Geography settings\n",
    "seed_geography = \"STAAT\"  # Should usually not be changed\n",
    "\n",
    "# Household column - set this AFTER running Step 1 to see available columns\n",
    "# This column from your control totals data will be used as the number of households\n",
    "household_column = None  # e.g., \"Insgesamt_Bevoelkerung\" or \"numberOfHouseholds\"\n",
    "\n",
    "# MiD (seed data) filtering settings\n",
    "filter_mid = False  # If True, filters MiD by the criteria below\n",
    "kernwo = 2          # Day of week: 1=Monday, 2=Tuesday-Thursday, 3=Friday, 4=Saturday-Sunday\n",
    "regiostar17 = [121, 123, 124]  # Regional types - see https://bmdv.bund.de/SharedDocs/DE/Artikel/G/regionalstatistische-raumtypologie.html\n",
    "\n",
    "# =============================================================================\n",
    "# END USER CONFIGURATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize and Inspect Data\n",
    "\n",
    "This step reads the geo crosswalk, creates the control totals template, and shows you the available columns so you can set `household_column` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "\n",
    "# Read first cell from crosswalk CSV to determine lowest geography name\n",
    "crosswalk_path = 'data/_prep0_geo_cross_walk.csv'\n",
    "lowest_geography_name = unidecode(pd.read_csv(crosswalk_path, header=None).iloc[0, 0])\n",
    "\n",
    "print(f\"Lowest geography name: {lowest_geography_name}\")\n",
    "\n",
    "# Save updated (unidecoded) value back to the CSV\n",
    "df_cross = pd.read_csv(crosswalk_path)\n",
    "df_cross.rename(columns={df_cross.columns[0]: lowest_geography_name}, inplace=True)\n",
    "\n",
    "# Auto-calculate ZENSUS1km from ZENSUS100m if the column exists\n",
    "if \"ZENSUS1km\" in df_cross.columns and \"ZENSUS100m\" in df_cross.columns:\n",
    "    df_cross[\"ZENSUS1km\"] = df_cross[\"ZENSUS100m\"].str.replace(\n",
    "        r\"100mN(\\d{5})E(\\d{5})\",\n",
    "        lambda m: f\"1kmN{int(m.group(1))//10}E{int(m.group(2))//10}\",\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "df_cross.to_csv(crosswalk_path, index=False)\n",
    "\n",
    "output_filename = f'data/_prep1_control_totals_{lowest_geography_name}.csv'\n",
    "\n",
    "if not os.path.exists(output_filename):\n",
    "    # Create empty template - user will fill in or provide their own data\n",
    "    df_new = pd.DataFrame(columns=[lowest_geography_name])\n",
    "    df_new.to_csv(output_filename, index=False)\n",
    "    print(f\"Created empty template '{output_filename}'\")\n",
    "    print(\"Please fill this file with your control totals data, then re-run this cell.\")\n",
    "else:\n",
    "    print(f\"'{output_filename}' already exists.\")\n",
    "    df_existing = pd.read_csv(output_filename)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"AVAILABLE COLUMNS IN YOUR CONTROL TOTALS DATA:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for i, col in enumerate(df_existing.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nSet 'household_column' in the Configuration cell above to one of these columns.\")\n",
    "    print(\"This column should contain the number of households for each geographic unit.\")\n",
    "\n",
    "# Save vars to a config file\n",
    "config = {\n",
    "    \"lowest_geography_name\": lowest_geography_name,\n",
    "    \"output_everything\": output_everything,\n",
    "    \"seed_geography\": seed_geography,\n",
    "    \"filter_mid\": filter_mid,\n",
    "    \"kernwo\": kernwo,\n",
    "    \"regiostar17\": regiostar17,\n",
    "    \"household_column\": household_column,\n",
    "}\n",
    "with open(\"prep_config.json\", \"w\") as config_file:\n",
    "    json.dump(config, config_file)\n",
    "\n",
    "print(\"\\nConfiguration saved to prep_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fill Control Totals\n",
    "\n",
    "In the `_prep1_control_totals_{geography}.csv` file in the data folder, the control data of the lowest level (e.g., grid cell, building block, or even building) must be inserted.\n",
    "\n",
    "**Important:** Make sure `household_column` is set in the Configuration cell above before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reconcile and Create Templates\n",
    "\n",
    "It is possible that the raw data for the lowest level does not cover all blocks (or buildings, etc.) created in the geo-cross-walk, or that there are data for blocks that are not considered. This will be reconciled here.\n",
    "\n",
    "Then, templates for the controls of all levels will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Load vars\n",
    "with open(\"prep_config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "lowest_geography_name = config[\"lowest_geography_name\"]\n",
    "household_column = config[\"household_column\"]\n",
    "\n",
    "# Validate household_column is set\n",
    "if household_column is None:\n",
    "    raise ValueError(\n",
    "        \"household_column is not set! Please set it in the Configuration cell above \"\n",
    "        \"to specify which column contains the number of households.\"\n",
    "    )\n",
    "\n",
    "# Read the two CSV files into Pandas DataFrames\n",
    "df1 = pd.read_csv('data/_prep0_geo_cross_walk.csv')\n",
    "df2 = pd.read_csv(f'data/_prep1_control_totals_{lowest_geography_name}.csv')\n",
    "\n",
    "# Validate household_column exists in control totals\n",
    "if household_column not in df2.columns:\n",
    "    raise ValueError(\n",
    "        f\"household_column '{household_column}' not found in control totals data. \"\n",
    "        f\"Available columns: {list(df2.columns)}\"\n",
    "    )\n",
    "\n",
    "# Rename household column to numberOfHouseholds for PopSim compatibility\n",
    "if household_column != \"numberOfHouseholds\":\n",
    "    df2 = df2.rename(columns={household_column: \"numberOfHouseholds\"})\n",
    "    print(f\"Renamed '{household_column}' to 'numberOfHouseholds' for PopSim compatibility.\")\n",
    "\n",
    "# Drop duplicates from each DataFrame\n",
    "len1_before, len2_before = len(df1), len(df2)\n",
    "df1 = df1.drop_duplicates(subset=[lowest_geography_name])\n",
    "df2 = df2.drop_duplicates(subset=[lowest_geography_name])\n",
    "len1_after, len2_after = len(df1), len(df2)\n",
    "print(f\"Removed {len1_before - len1_after} duplicates from geo_cross_walk\")\n",
    "print(f\"Removed {len2_before - len2_after} duplicates from control_totals_{lowest_geography_name}\")\n",
    "\n",
    "# Extract the common values\n",
    "common_values = set(df1[lowest_geography_name]).intersection(df2[lowest_geography_name])\n",
    "\n",
    "# Filter DataFrames\n",
    "filtered_df1 = df1[df1[lowest_geography_name].isin(common_values)]\n",
    "filtered_df2 = df2[df2[lowest_geography_name].isin(common_values)]\n",
    "\n",
    "# Sort both by lowest geography\n",
    "filtered_df1 = filtered_df1.sort_values(by=lowest_geography_name).reset_index(drop=True)\n",
    "filtered_df2 = filtered_df2.sort_values(by=lowest_geography_name).reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of rows in _prep0_geo_cross_walk: {len(df1)}\")\n",
    "print(f\"Number of rows in geo_cross_walk (i.e. given geo-cells that actually have values): {len(filtered_df1)}\")\n",
    "print(f\"Removed {len(df1) - len(filtered_df1)} rows\")\n",
    "\n",
    "print(f\"Number of rows in _prep1_control_totals_{lowest_geography_name}: {len(df2)}\")\n",
    "print(f\"Number of rows in control_totals_{lowest_geography_name} (i.e. control_totals that apply to the given geo-cells): {len(filtered_df2)}\")\n",
    "print(f\"Removed {len(df2) - len(filtered_df2)} rows\")\n",
    "\n",
    "\n",
    "def unidecode_and_clean_column_names(df):\n",
    "    modified_columns = {}\n",
    "    for col in df.columns:\n",
    "        clean = unidecode(col).replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "        modified_columns[col] = clean\n",
    "    df.rename(columns=modified_columns, inplace=True)\n",
    "\n",
    "\n",
    "unidecode_and_clean_column_names(filtered_df1)\n",
    "unidecode_and_clean_column_names(filtered_df2)\n",
    "\n",
    "# Confirm keys match\n",
    "if not np.array_equal(filtered_df1[lowest_geography_name].values, filtered_df2[lowest_geography_name].values):\n",
    "    raise ValueError(\"Mismatch between keys in geo_cross_walk and control totals. Aborting.\")\n",
    "\n",
    "# Save cleaned geo_cross_walk\n",
    "filtered_df1.to_csv('data/geo_cross_walk.csv', index=False)\n",
    "print(f\"Created data/geo_cross_walk.csv\")\n",
    "\n",
    "# Create control_total templates for all higher-level geographies\n",
    "geo_names = list(filtered_df1.columns)\n",
    "for i, geo_name in enumerate(geo_names):\n",
    "    if i == 0:\n",
    "        continue  # Skip lowest geography\n",
    "\n",
    "    data = filtered_df1.iloc[:, i:].drop_duplicates(subset=geo_name)\n",
    "    if geo_name == \"ZENSUS1km\":\n",
    "        # Populate ZENSUS1km from census data if available\n",
    "        census1km_path = \"data/ZENSUS1km.csv\"\n",
    "        if os.path.exists(census1km_path):\n",
    "            census1km_df = pd.read_csv(census1km_path)\n",
    "            data = data.merge(census1km_df, how=\"left\", left_on=geo_name, right_on=\"ZENSUS1km\")\n",
    "    output_filename = f'data/_prep2_control_totals_{geo_name}.csv'\n",
    "    if not os.path.exists(output_filename):\n",
    "        data.to_csv(output_filename, index=False)\n",
    "        print(f\"Created {output_filename}\")\n",
    "    else:\n",
    "        print(f\"{output_filename} already exists, not overwritten.\")\n",
    "\n",
    "# Create finished control_total for lowest geography\n",
    "merged = pd.concat([filtered_df1, filtered_df2.iloc[:, 1:]], axis=1)\n",
    "\n",
    "for col in merged.columns:\n",
    "    if col not in geo_names:\n",
    "        merged.rename(columns={col: f\"{col}_{lowest_geography_name}\"}, inplace=True)\n",
    "merged = merged.fillna(0)\n",
    "merged.to_csv(f'data/control_totals_{lowest_geography_name}.csv', index=False)\n",
    "print(f\"Created data/control_totals_{lowest_geography_name}.csv\")\n",
    "\n",
    "# Update config with geo names\n",
    "config[\"geo_names\"] = geo_names\n",
    "with open(\"prep_config.json\", \"w\") as config_file:\n",
    "    json.dump(config, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fill Higher-Level Controls\n",
    "\n",
    "Now the `_prep2_*` files for the remaining geographies must be filled with data by the user (if using only census, just press play). The prepared data must not be overwritten.\n",
    "\n",
    "The settings file is fitted, and the controls file prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "# Load vars\n",
    "with open(\"prep_config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "lowest_geography_name = config[\"lowest_geography_name\"]\n",
    "geo_names = config[\"geo_names\"]\n",
    "output_everything = config[\"output_everything\"]\n",
    "seed_geography = config[\"seed_geography\"]\n",
    "\n",
    "\n",
    "def unidecode_and_clean_column_names(df):\n",
    "    modified_columns = {}\n",
    "    for unclean_column_name in df.columns:\n",
    "        modified_name = unidecode(unclean_column_name)\n",
    "        modified_name = modified_name.replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \"\")\n",
    "        modified_columns[unclean_column_name] = modified_name\n",
    "    df.rename(columns=modified_columns, inplace=True)\n",
    "\n",
    "\n",
    "# Open all control_total files\n",
    "control_total_files = [file for file in os.listdir('data') if\n",
    "                       file.startswith('_prep2_control_totals_') or file.startswith('_prep1_control_totals_')]\n",
    "\n",
    "# Create a dictionary to store data for the new control structure\n",
    "new_data = {\n",
    "    'target': [],\n",
    "    'geography': [],\n",
    "    'seed_table': [],\n",
    "    'importance': [],\n",
    "    'control_field': [],\n",
    "    'expression': []\n",
    "}\n",
    "\n",
    "# Ensure the number of files matches the geographies\n",
    "if len(control_total_files) != len(geo_names):\n",
    "    raise ValueError(\"The number of control_total files does not match the number of geographies. Aborting.\")\n",
    "\n",
    "total_hh_control = None\n",
    "\n",
    "# Process each control_total file\n",
    "for file in control_total_files:\n",
    "    df = pd.read_csv(f'data/{file}')\n",
    "    unidecode_and_clean_column_names(df)\n",
    "\n",
    "    parts = file.split('_')\n",
    "    if len(parts) > 3:\n",
    "        geography_name = parts[4].split('.')[0]\n",
    "        if geography_name != df.columns[0] or geography_name not in geo_names:\n",
    "            raise ValueError(\"File or file name does not match the expected format. Aborting.\")\n",
    "    else:\n",
    "        raise ValueError(\"File name does not match the expected format. Aborting.\")\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        if column_name not in geo_names:\n",
    "            new_name = f'{column_name}_{geography_name}'\n",
    "            df.rename(columns={column_name: new_name}, inplace=True)\n",
    "\n",
    "            if new_name.startswith(\"numberOfHouseholds\") and geography_name == lowest_geography_name:\n",
    "                total_hh_control = f'{new_name}_target'\n",
    "\n",
    "            new_data['target'].append(f'{new_name}_target')\n",
    "            new_data['geography'].append(geography_name)\n",
    "            new_data['control_field'].append(new_name)\n",
    "\n",
    "    # Save as cleaned file if not already existing\n",
    "    output_clean_name = f'data/{file.replace(\"_prep2_\", \"\")}'\n",
    "    if not os.path.exists(output_clean_name):\n",
    "        df = df.fillna(0)\n",
    "        df.to_csv(output_clean_name, index=False)\n",
    "        print(f\"Created {output_clean_name}\")\n",
    "    else:\n",
    "        print(f\"{output_clean_name} already exists, not overwritten.\")\n",
    "\n",
    "if total_hh_control is None:\n",
    "    raise ValueError(\"Could not find a total_hh_control. Aborting.\")\n",
    "\n",
    "# Insert example expression\n",
    "new_data['expression'].append('(households.H_GEW > 0) & (households.H_GEW < np.inf)')\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame({key: pd.Series(value) for key, value in new_data.items()})\n",
    "output_file_name = os.path.join('configs', '_prep3_controls.csv')\n",
    "if not os.path.exists(output_file_name):\n",
    "    df.to_csv(output_file_name, index=False)\n",
    "    print(f\"Created controls file template '{output_file_name}'.\")\n",
    "else:\n",
    "    print(f\"Controls file template '{output_file_name}' already exists, not overwritten.\")\n",
    "\n",
    "# Edit the YAML settings file ----------------------------------------------\n",
    "popsim_settings_file_name = os.path.join('configs', 'settings.yaml')\n",
    "with open(popsim_settings_file_name, \"r\") as yaml_file:\n",
    "    popsim_settings = yaml.safe_load(yaml_file)\n",
    "\n",
    "topdown_geos = geo_names[::-1]\n",
    "popsim_settings[\"geographies\"] = topdown_geos\n",
    "popsim_settings[\"seed_geography\"] = seed_geography\n",
    "popsim_settings[\"total_hh_control\"] = total_hh_control\n",
    "\n",
    "# Trim input_table_list after geo_cross_walk\n",
    "index_to_remove_from = None\n",
    "for i, table in enumerate(popsim_settings[\"input_table_list\"]):\n",
    "    if table[\"tablename\"] == \"geo_cross_walk\":\n",
    "        index_to_remove_from = i\n",
    "\n",
    "if index_to_remove_from is not None:\n",
    "    popsim_settings[\"input_table_list\"] = popsim_settings[\"input_table_list\"][:index_to_remove_from + 1]\n",
    "else:\n",
    "    raise ValueError(\"Could not find 'geo_cross_walk' in the input_table_list. Aborting.\")\n",
    "\n",
    "# Add new control table entries\n",
    "for geo_name in geo_names:\n",
    "    tablename = f\"{geo_name}_control_data\"\n",
    "    filename = f\"control_totals_{geo_name}.csv\"\n",
    "    popsim_settings[\"input_table_list\"].append({\n",
    "        \"tablename\": tablename,\n",
    "        \"filename\": filename\n",
    "    })\n",
    "\n",
    "# Output tables\n",
    "if output_everything:\n",
    "    popsim_settings[\"output_tables\"][\"action\"] = \"skip\"\n",
    "    popsim_settings[\"output_tables\"][\"tables\"] = \"geo_cross_walk\"\n",
    "else:\n",
    "    popsim_settings[\"output_tables\"][\"action\"] = \"include\"\n",
    "    popsim_settings[\"output_tables\"][\"tables\"] = [\"expanded_household_ids\"]\n",
    "    for geo_name in geo_names:\n",
    "        popsim_settings[\"output_tables\"][\"tables\"].append(f\"summary_{geo_name}\")\n",
    "    for geo_name in geo_names[:-2]:\n",
    "        popsim_settings[\"output_tables\"][\"tables\"].append(f\"summary_{geo_name}_{seed_geography}\")\n",
    "\n",
    "# Models\n",
    "popsim_settings[\"models\"] = [m for m in popsim_settings[\"models\"] if \"sub_balancing\" not in m]\n",
    "integerize_index = popsim_settings[\"models\"].index(\"integerize_final_seed_weights\")\n",
    "for geo_name in geo_names[:-2]:\n",
    "    popsim_settings[\"models\"].insert(integerize_index + 1, f\"sub_balancing.geography={geo_name}\")\n",
    "\n",
    "with open(popsim_settings_file_name, \"w\") as yaml_file:\n",
    "    yaml.dump(popsim_settings, yaml_file, default_flow_style=False)\n",
    "print(\"Modified settings.yaml file.\")\n",
    "\n",
    "# Edit analysis settings file ----------------------------------------------\n",
    "analysis_settings_file_name = os.path.join('scripts', 'verification.yaml')\n",
    "with open(analysis_settings_file_name, \"r\") as yaml_file:\n",
    "    analysis_settings = yaml.safe_load(yaml_file)\n",
    "\n",
    "analysis_settings[\"group_geographies\"] = topdown_geos\n",
    "try:\n",
    "    this_folder_name = os.path.basename(os.path.dirname(os.path.abspath(__file__)))\n",
    "except Exception:\n",
    "    this_folder_name = os.path.basename(os.getcwd())\n",
    "analysis_settings[\"region\"] = this_folder_name\n",
    "\n",
    "analysis_settings[\"seed_cols\"][\"geog\"] = seed_geography\n",
    "analysis_settings[\"summaries\"] = [f\"output/final_summary_{geo}.csv\" for geo in geo_names]\n",
    "analysis_settings[\"summaries\"] += [f\"output/final_summary_{geo}_{seed_geography}.csv\" for geo in geo_names[:-2]]\n",
    "\n",
    "analysis_settings[\"aggregate_summaries\"] = [\n",
    "    {\n",
    "        'name': control_field,\n",
    "        'geography': geography,\n",
    "        'control': f'{target}_control',\n",
    "        'result': f'{target}_result'\n",
    "    }\n",
    "    for target, geography, control_field in zip(new_data['target'], new_data['geography'], new_data['control_field'])\n",
    "]\n",
    "\n",
    "with open(analysis_settings_file_name, \"w\") as yaml_file:\n",
    "    yaml.dump(analysis_settings, yaml_file, default_flow_style=False)\n",
    "print(\"Modified verification.yaml file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Edit Controls\n",
    "\n",
    "The user now has to edit the `_prep3_controls.csv` file in the configs folder. Add expressions and importance; seed_table is added automatically. The script will then filter the seed files accordingly and check for implausibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Load config\n",
    "with open(\"prep_config.json\", \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "filter_mid = config[\"filter_mid\"]\n",
    "kernwo = config[\"kernwo\"]\n",
    "regiostar17 = config[\"regiostar17\"]\n",
    "\n",
    "# Load controls\n",
    "controls_file_name = os.path.join('configs', '_prep3_controls.csv')\n",
    "controls_df = pd.read_csv(controls_file_name)\n",
    "\n",
    "# Load seed data\n",
    "seed_persons_df = pd.read_csv('data/MiD2017_Personen.csv', sep=\";\")\n",
    "seed_households_df = pd.read_csv('data/MiD2017_Haushalte.csv', sep=\";\")\n",
    "\n",
    "# Filter MiD according to specs\n",
    "if filter_mid:\n",
    "    seed_persons_df = seed_persons_df[\n",
    "        (seed_persons_df['RegioStaR17'].isin(regiostar17))\n",
    "    ]\n",
    "    seed_households_df = seed_households_df[\n",
    "        seed_households_df['RegioStaR17'].isin(regiostar17)\n",
    "    ]\n",
    "\n",
    "print(f'Columns of seed_persons: {seed_persons_df.columns}')\n",
    "print(f'Columns of seed_households: {seed_households_df.columns}')\n",
    "\n",
    "# Define essential columns\n",
    "column_names_to_include = {'H_ID', 'H_GEW', 'HP_ID', 'P_ID', 'P_GEW'}\n",
    "unique_column_names = column_names_to_include.copy()\n",
    "\n",
    "# Extract column names from expressions\n",
    "pattern = r'\\.(?P<column_name>[^ ]+)'\n",
    "for expression in controls_df['expression']:\n",
    "    if pd.isna(expression) or (isinstance(expression, float) and math.isnan(expression)):\n",
    "        raise ValueError(\n",
    "            f\"Empty or NaN expression encountered. Make sure to write expressions for all targets in {controls_file_name} and rerun. Aborting.\"\n",
    "        )\n",
    "\n",
    "    matches = re.finditer(pattern, expression)\n",
    "    for match in matches:\n",
    "        column_name = match.group('column_name')\n",
    "        unique_column_names.add(column_name)\n",
    "\n",
    "# Filter columns\n",
    "seed_persons_df = seed_persons_df[list(unique_column_names.intersection(seed_persons_df.columns))]\n",
    "seed_households_df = seed_households_df[list(unique_column_names.intersection(seed_households_df.columns))]\n",
    "\n",
    "print(f'Columns of seed_persons after filtering: {seed_persons_df.columns}')\n",
    "print(f'Columns of seed_households after filtering: {seed_households_df.columns}')\n",
    "\n",
    "# Add constant geography\n",
    "seed_persons_df['STAAT'] = 1\n",
    "seed_households_df['STAAT'] = 1\n",
    "\n",
    "# Save outputs\n",
    "seed_persons_df.to_csv('data/seed_persons.csv', index=False)\n",
    "print(\"Created data/seed_persons.csv\")\n",
    "seed_households_df.to_csv('data/seed_households.csv', index=False)\n",
    "print(\"Created data/seed_households.csv\")\n",
    "controls_df.to_csv('configs/controls.csv', index=False)\n",
    "print(\"Created configs/controls.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup (Optional)\n",
    "\n",
    "Clean up all unnecessary intermediate files that were created only for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def delete_files(folders, prefix, extension):\n",
    "    \"\"\"Delete files matching prefix and extension in specified folders (not subfolders).\"\"\"\n",
    "    for folder in folders:\n",
    "        if os.path.exists(folder):\n",
    "            files_in_folder = [file for file in os.listdir(folder) if\n",
    "                               file.startswith(prefix) and file.endswith(extension)]\n",
    "            for file in files_in_folder:\n",
    "                file_path = os.path.join(folder, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Folder not found: {folder}\")\n",
    "\n",
    "\n",
    "folders_to_process = ['data', 'configs', 'configs_mp']\n",
    "\n",
    "# Uncomment the lines below to delete intermediate files\n",
    "# delete_files(folders_to_process, '_prep', '.csv')  # Delete all _prep*.csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PopSim\n",
    "\n",
    "Use in anaconda console:\n",
    "```\n",
    "conda activate popsim\n",
    "run_populationsim.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
