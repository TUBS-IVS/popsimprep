{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PopSim Data Preparation (Census Mode)\n\nThis notebook prepares all input files for PopulationSim using German Census grid data.\n\n## Requirements\n\n1. **GeoPackage** (`.gpkg`) - polygon defining your study area boundary\n2. **Census data** (parquet or CSV) - 100m and 1km grid cells with population attributes\n3. **MiD seed data** (CSV) - household and person survey data (`MiD2017_Haushalte.csv`, `MiD2017_Personen.csv`)\n\n## Census Data Format\n\nCensus files must have cell IDs in the **first column** matching this format:\n- **100m**: `CRS3035RES100mN{northing}E{easting}` (e.g., `CRS3035RES100mN2689100E4337000`)\n- **1km**: `CRS3035RES1000mN{northing}E{easting}` (e.g., `CRS3035RES1000mN2689000E4337000`)\n\nCoordinates are **EPSG:3035** (ETRS89-extended / LAEA Europe). All other columns become available as control totals.\n\n## What Gets Generated\n\n- `geo_cross_walk.csv` - geographic hierarchy (ZENSUS100m → ZENSUS1km → STAAT → WELT)\n- `control_totals_*.csv` - census data formatted as control totals\n- `seed_persons.csv`, `seed_households.csv` - filtered MiD data\n- `configs/controls.csv` - control definitions for PopSim\n- Updates to `settings.yaml` and `verification.yaml`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\n**Edit all paths and settings below before running.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# USER CONFIGURATION\n# =============================================================================\n\n# --- Study Area ---\ngeopackage_path = \"data/outlineNIwithBremen.gpkg\"\ngeopackage_crs = \"EPSG:25832\"  # Set CRS if not embedded (None = auto-detect)\n\n# --- Census Data (parquet or CSV) ---\ncensus_100m_path = \"data/cells_100m_with_gender_backf_binneds_happyorphans.parquet\"\ncensus_1km_path = \"data/cells_1km_with_binneds.parquet\"\n\n# Column containing number of households (run Step 1 with None to see options)\nhousehold_column = None  # e.g., \"Insgesamt_Haushalte_100m-Gitter\"\n\n# --- MiD Seed Data (semicolon-separated CSVs) ---\nmid_households_path = \"data/MiD2017_Haushalte.csv\"\nmid_persons_path = \"data/MiD2017_Personen.csv\"\n\n# --- MiD Filtering (optional) ---\nfilter_mid = False  # Set True to apply filters below\nkernwo = [2, 3]  # Day of week: 1=Monday, 2=Tue-Thu, 3=Friday, 4=Sat-Sun\nregiostar17 = [121, 123, 124]  # Regional types to include\n\n# --- CSV Separators ---\ncensus_csv_sep = \";\"   # For input CSVs (ignored for parquet)\nintermediate_sep = \",\"  # For intermediate files (use \";\" for German Excel)\n# Note: Final PopSim files are always comma-separated\n\n# --- Advanced ---\noutput_everything = False  # True = output all PopSim intermediates\nseed_geography = \"STAAT\"   # Geography level for seed data (usually unchanged)\n\n# =============================================================================\n# END CONFIGURATION\n# ============================================================================="
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Study Area and Filter Census\n",
    "\n",
    "Loads your GeoPackage, filters census cells to the study area, and shows available columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nimport re\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import box\n\nprint(\"[Step 1/4] Loading study area and filtering census...\")\nprint(\"=\" * 60)\n\n# Load GeoPackage\nprint(f\"Loading GeoPackage: {geopackage_path}\")\nstudy_area = gpd.read_file(geopackage_path)\n\n# Handle CRS\nif study_area.crs is None and geopackage_crs:\n    study_area = study_area.set_crs(geopackage_crs)\n    print(f\"  Set CRS to: {geopackage_crs}\")\nelif study_area.crs is None:\n    raise ValueError(\"GeoPackage has no CRS. Please set geopackage_crs in configuration.\")\n\n# Transform to EPSG:3035 (Census CRS)\nstudy_area_3035 = study_area.to_crs(\"EPSG:3035\")\nbounds = study_area_3035.total_bounds  # minx, miny, maxx, maxy\nprint(f\"  Study area bounds (EPSG:3035): {bounds}\")\n\n# Parse cell ID to extract coordinates\ndef parse_cell_id_100m(cell_id):\n    \"\"\"Extract N,E coordinates from 100m cell ID like CRS3035RES100mN2689100E4337000\"\"\"\n    match = re.match(r'CRS3035RES100mN(\\d+)E(\\d+)', str(cell_id))\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None, None\n\ndef parse_cell_id_1km(cell_id):\n    \"\"\"Extract N,E coordinates from 1km cell ID like CRS3035RES1000mN2689000E4337000\"\"\"\n    match = re.match(r'CRS3035RES1000mN(\\d+)E(\\d+)', str(cell_id))\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None, None\n\ndef get_1km_id_from_100m(cell_id_100m):\n    \"\"\"Convert 100m cell ID to corresponding 1km cell ID.\"\"\"\n    n, e = parse_cell_id_100m(cell_id_100m)\n    if n is None:\n        return None\n    # Round down to nearest 1000m\n    n_1km = (n // 1000) * 1000\n    e_1km = (e // 1000) * 1000\n    return f\"CRS3035RES1000mN{n_1km}E{e_1km}\"\n\n# Load 100m census\nprint(f\"\\nLoading 100m census: {census_100m_path}\")\n\nif census_100m_path.endswith('.parquet'):\n    import pyarrow.parquet as pq\n    pf_100m = pq.ParquetFile(census_100m_path)\n    print(f\"  Total rows: {pf_100m.metadata.num_rows:,}\")\n    print(f\"  Total columns: {pf_100m.metadata.num_columns}\")\n    \n    # Read in batches to filter efficiently\n    print(\"  Filtering to study area (this may take a moment)...\")\n    filtered_chunks = []\n    total_read = 0\n    \n    for batch in pf_100m.iter_batches(batch_size=100000):\n        df_batch = batch.to_pandas()\n        total_read += len(df_batch)\n        \n        # Parse coordinates and filter\n        coords = df_batch.iloc[:, 0].apply(parse_cell_id_100m)\n        df_batch['_N'] = coords.apply(lambda x: x[0])\n        df_batch['_E'] = coords.apply(lambda x: x[1])\n        \n        # Bounding box filter\n        mask = (\n            (df_batch['_N'] >= bounds[1]) & (df_batch['_N'] <= bounds[3]) &\n            (df_batch['_E'] >= bounds[0]) & (df_batch['_E'] <= bounds[2])\n        )\n        df_filtered = df_batch[mask].drop(columns=['_N', '_E'])\n        \n        if len(df_filtered) > 0:\n            filtered_chunks.append(df_filtered)\n        \n        if total_read % 500000 == 0:\n            print(f\"    Processed {total_read:,} rows...\")\n    \n    census_100m = pd.concat(filtered_chunks, ignore_index=True)\nelse:\n    # CSV - load all at once (usually smaller files)\n    print(f\"  Loading CSV with separator: '{census_csv_sep}'\")\n    census_100m_full = pd.read_csv(census_100m_path, sep=census_csv_sep)\n    print(f\"  Total rows: {len(census_100m_full):,}\")\n    \n    # Filter by bounding box\n    coords = census_100m_full.iloc[:, 0].apply(parse_cell_id_100m)\n    census_100m_full['_N'] = coords.apply(lambda x: x[0])\n    census_100m_full['_E'] = coords.apply(lambda x: x[1])\n    \n    mask = (\n        (census_100m_full['_N'] >= bounds[1]) & (census_100m_full['_N'] <= bounds[3]) &\n        (census_100m_full['_E'] >= bounds[0]) & (census_100m_full['_E'] <= bounds[2])\n    )\n    census_100m = census_100m_full[mask].drop(columns=['_N', '_E']).copy()\n\nprint(f\"  Filtered to {len(census_100m):,} cells in bounding box\")\n\n# Fine filter: check actual intersection with study area polygon\nprint(\"  Performing precise polygon intersection...\")\nid_col = census_100m.columns[0]\n\ndef cell_intersects_study_area(cell_id):\n    n, e = parse_cell_id_100m(cell_id)\n    if n is None:\n        return False\n    cell_geom = box(e, n, e + 100, n + 100)\n    return study_area_3035.geometry.intersects(cell_geom).any()\n\n# Sample check - if bbox is tight, skip full intersection\nsample_mask = census_100m[id_col].sample(min(100, len(census_100m))).apply(cell_intersects_study_area)\nif sample_mask.mean() > 0.9:\n    print(\"  Bounding box is tight, skipping detailed intersection.\")\nelse:\n    mask = census_100m[id_col].apply(cell_intersects_study_area)\n    census_100m = census_100m[mask]\n    print(f\"  After polygon intersection: {len(census_100m):,} cells\")\n\n# Show available columns for household selection\nprint(f\"\\n{'='*60}\")\nprint(\"AVAILABLE COLUMNS IN 100m CENSUS DATA:\")\nprint(f\"{'='*60}\")\n\n# Find likely household columns\nhh_keywords = ['haushalt', 'household', 'hh_', 'wohnung']\nsuggested = []\nfor col in census_100m.columns:\n    col_lower = col.lower()\n    if any(kw in col_lower for kw in hh_keywords):\n        suggested.append(col)\n\nif suggested:\n    print(\"\\nSUGGESTED HOUSEHOLD COLUMNS:\")\n    for col in suggested[:10]:\n        print(f\"  -> {col}\")\n\nprint(f\"\\nALL COLUMNS ({len(census_100m.columns)}):\")\nfor i, col in enumerate(census_100m.columns):\n    if i < 30:\n        print(f\"  {col}\")\n    elif i == 30:\n        print(f\"  ... and {len(census_100m.columns) - 30} more\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA PREVIEW (first 3 rows, first 5 columns):\")\nprint(f\"{'='*60}\")\nprint(census_100m.iloc[:3, :5].to_string())\n\n# Load 1km census\nprint(f\"\\n{'='*60}\")\nprint(f\"Loading 1km census: {census_1km_path}\")\n\nif census_1km_path.endswith('.parquet'):\n    census_1km_full = pd.read_parquet(census_1km_path)\nelse:\n    census_1km_full = pd.read_csv(census_1km_path, sep=census_csv_sep)\nprint(f\"  Total rows: {len(census_1km_full):,}\")\n\n# Filter 1km by deriving from 100m cells\nkm_ids_needed = set(census_100m[id_col].apply(get_1km_id_from_100m).dropna())\nkm_id_col = census_1km_full.columns[1]  # Usually GITTER_ID_1km\ncensus_1km = census_1km_full[census_1km_full[km_id_col].isin(km_ids_needed)].copy()\nprint(f\"  Filtered to {len(census_1km):,} 1km cells\")\n\n# Save filtered data as parquet (always - most efficient for intermediates)\ncensus_100m.to_parquet('data/_census_100m_filtered.parquet', index=False)\ncensus_1km.to_parquet('data/_census_1km_filtered.parquet', index=False)\nprint(f\"\\nSaved filtered census to data/_census_*_filtered.parquet\")\n\n# Normalize kernwo to list\nkernwo_list = kernwo if isinstance(kernwo, list) else [kernwo]\n\n# Save config (including MiD paths)\nconfig = {\n    \"geopackage_path\": geopackage_path,\n    \"census_100m_id_col\": id_col,\n    \"census_1km_id_col\": km_id_col,\n    \"household_column\": household_column,\n    \"mid_households_path\": mid_households_path,\n    \"mid_persons_path\": mid_persons_path,\n    \"output_everything\": output_everything,\n    \"seed_geography\": seed_geography,\n    \"filter_mid\": filter_mid,\n    \"kernwo\": kernwo_list,\n    \"regiostar17\": regiostar17,\n    \"intermediate_sep\": intermediate_sep,\n    \"num_100m_cells\": len(census_100m),\n    \"num_1km_cells\": len(census_1km),\n}\nwith open(\"prep_config.json\", \"w\") as f:\n    json.dump(config, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(\"SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"  100m cells in study area: {len(census_100m):,}\")\nprint(f\"  1km cells in study area: {len(census_1km):,}\")\nprint(f\"  MiD households: {mid_households_path}\")\nprint(f\"  MiD persons: {mid_persons_path}\")\nprint(f\"\\nSet 'household_column' in Configuration and re-run Step 1,\")\nprint(\"or proceed to Step 2 if already set.\")\nprint(\"\\n[Step 1/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Generate Geo Crosswalk and Control Totals\n\nCreates the geographic hierarchy and control totals from filtered census data:\n- `geo_cross_walk.csv` - mapping ZENSUS100m → ZENSUS1km → STAAT → WELT\n- `control_totals_*.csv` - one file per geography level"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from unidecode import unidecode\n",
    "\n",
    "print(\"[Step 2/4] Generating geo crosswalk and control totals...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load config\n",
    "with open(\"prep_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "household_column = config[\"household_column\"]\n",
    "id_col_100m = config[\"census_100m_id_col\"]\n",
    "id_col_1km = config[\"census_1km_id_col\"]\n",
    "output_everything = config[\"output_everything\"]\n",
    "seed_geography = config[\"seed_geography\"]\n",
    "intermediate_sep = config.get(\"intermediate_sep\", \",\")  # Default to comma\n",
    "\n",
    "if household_column is None:\n",
    "    raise ValueError(\"household_column not set! Set it in Configuration and re-run Step 1.\")\n",
    "\n",
    "# Load filtered census\n",
    "census_100m = pd.read_parquet('data/_census_100m_filtered.parquet')\n",
    "census_1km = pd.read_parquet('data/_census_1km_filtered.parquet')\n",
    "\n",
    "print(f\"Loaded {len(census_100m):,} 100m cells, {len(census_1km):,} 1km cells\")\n",
    "\n",
    "# Validate household column\n",
    "if household_column not in census_100m.columns:\n",
    "    raise ValueError(f\"household_column '{household_column}' not found in census data.\")\n",
    "\n",
    "# Check household values\n",
    "hh_values = census_100m[household_column]\n",
    "if hh_values.isna().any():\n",
    "    na_count = hh_values.isna().sum()\n",
    "    print(f\"WARNING: {na_count} cells have missing household values (will be set to 0)\")\n",
    "if (hh_values < 0).any():\n",
    "    neg_count = (hh_values < 0).sum()\n",
    "    print(f\"WARNING: {neg_count} cells have negative household values\")\n",
    "\n",
    "# Helper to get 1km ID from 100m ID\n",
    "def get_1km_from_100m(cell_id):\n",
    "    \"\"\"Convert 100m cell ID to corresponding 1km cell ID.\"\"\"\n",
    "    match = re.match(r'CRS3035RES100mN(\\d+)E(\\d+)', str(cell_id))\n",
    "    if match:\n",
    "        n, e = int(match.group(1)), int(match.group(2))\n",
    "        # Round down to nearest 1000m\n",
    "        n_1km = (n // 1000) * 1000\n",
    "        e_1km = (e // 1000) * 1000\n",
    "        return f\"CRS3035RES1000mN{n_1km}E{e_1km}\"\n",
    "    return None\n",
    "\n",
    "# Standardize column names\n",
    "def clean_col_name(name):\n",
    "    return unidecode(name).replace(\" \", \"\").replace(\".\", \"\").replace(\",\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "# Rename columns\n",
    "census_100m.columns = [clean_col_name(c) for c in census_100m.columns]\n",
    "census_1km.columns = [clean_col_name(c) for c in census_1km.columns]\n",
    "household_column_clean = clean_col_name(household_column)\n",
    "\n",
    "# Find the ID columns after cleaning\n",
    "id_col_100m_clean = census_100m.columns[0]\n",
    "id_col_1km_clean = [c for c in census_1km.columns if '1km' in c.lower()][0]\n",
    "\n",
    "# Create geo_cross_walk (hierarchy: ZENSUS100m -> ZENSUS1km -> STAAT -> WELT)\n",
    "print(\"\\nCreating geo_cross_walk...\")\n",
    "geo_cross = pd.DataFrame()\n",
    "geo_cross['ZENSUS100m'] = census_100m[id_col_100m_clean]\n",
    "geo_cross['ZENSUS1km'] = geo_cross['ZENSUS100m'].apply(get_1km_from_100m)\n",
    "geo_cross['STAAT'] = 1\n",
    "geo_cross['WELT'] = 1\n",
    "\n",
    "geo_cross.to_csv('data/geo_cross_walk.csv', index=False)  # Final - always comma\n",
    "print(f\"  Created data/geo_cross_walk.csv ({len(geo_cross)} rows)\")\n",
    "\n",
    "# Create control_totals for 100m (lowest level)\n",
    "print(\"\\nCreating control totals...\")\n",
    "\n",
    "# Geography names (hierarchy from lowest to highest)\n",
    "geo_names = ['ZENSUS100m', 'ZENSUS1km', 'STAAT', 'WELT']\n",
    "\n",
    "# Rename household column to numberOfHouseholds\n",
    "census_100m = census_100m.rename(columns={household_column_clean: 'numberOfHouseholds'})\n",
    "\n",
    "# Add geography columns\n",
    "census_100m = census_100m.rename(columns={id_col_100m_clean: 'ZENSUS100m'})\n",
    "census_100m['ZENSUS1km'] = census_100m['ZENSUS100m'].apply(get_1km_from_100m)\n",
    "census_100m['STAAT'] = 1\n",
    "census_100m['WELT'] = 1\n",
    "\n",
    "# Suffix non-geo columns\n",
    "for col in census_100m.columns:\n",
    "    if col not in geo_names:\n",
    "        census_100m.rename(columns={col: f\"{col}_ZENSUS100m\"}, inplace=True)\n",
    "\n",
    "census_100m = census_100m.fillna(0)\n",
    "census_100m.to_csv('data/control_totals_ZENSUS100m.csv', index=False)  # Final - always comma\n",
    "print(f\"  Created data/control_totals_ZENSUS100m.csv\")\n",
    "\n",
    "# Create control_totals for 1km\n",
    "census_1km = census_1km.rename(columns={id_col_1km_clean: 'ZENSUS1km'})\n",
    "census_1km['STAAT'] = 1\n",
    "census_1km['WELT'] = 1\n",
    "\n",
    "for col in census_1km.columns:\n",
    "    if col not in geo_names:\n",
    "        census_1km.rename(columns={col: f\"{col}_ZENSUS1km\"}, inplace=True)\n",
    "\n",
    "census_1km = census_1km.fillna(0)\n",
    "census_1km.to_csv('data/control_totals_ZENSUS1km.csv', index=False)  # Final - always comma\n",
    "print(f\"  Created data/control_totals_ZENSUS1km.csv\")\n",
    "\n",
    "# Create control_totals for STAAT\n",
    "staat_df = pd.DataFrame({'STAAT': [1], 'WELT': [1]})\n",
    "staat_df.to_csv('data/control_totals_STAAT.csv', index=False)  # Final - always comma\n",
    "print(f\"  Created data/control_totals_STAAT.csv\")\n",
    "\n",
    "# Create control_totals for WELT (top level)\n",
    "welt_df = pd.DataFrame({'WELT': [1]})\n",
    "welt_df.to_csv('data/control_totals_WELT.csv', index=False)  # Final - always comma\n",
    "print(f\"  Created data/control_totals_WELT.csv\")\n",
    "\n",
    "# Create controls template\n",
    "print(\"\\nCreating controls template...\")\n",
    "controls_data = {\n",
    "    'target': [],\n",
    "    'geography': [],\n",
    "    'seed_table': [],\n",
    "    'importance': [],\n",
    "    'control_field': [],\n",
    "    'expression': []\n",
    "}\n",
    "\n",
    "total_hh_control = None\n",
    "\n",
    "# Add 100m controls\n",
    "for col in census_100m.columns:\n",
    "    if col not in geo_names:\n",
    "        controls_data['target'].append(f\"{col}_target\")\n",
    "        controls_data['geography'].append('ZENSUS100m')\n",
    "        controls_data['control_field'].append(col)\n",
    "        if col.startswith('numberOfHouseholds'):\n",
    "            total_hh_control = f\"{col}_target\"\n",
    "\n",
    "# Add 1km controls\n",
    "for col in census_1km.columns:\n",
    "    if col not in geo_names:\n",
    "        controls_data['target'].append(f\"{col}_target\")\n",
    "        controls_data['geography'].append('ZENSUS1km')\n",
    "        controls_data['control_field'].append(col)\n",
    "\n",
    "# Add example expression\n",
    "controls_data['expression'].append('(households.H_GEW > 0) & (households.H_GEW < np.inf)')\n",
    "\n",
    "controls_df = pd.DataFrame({k: pd.Series(v) for k, v in controls_data.items()})\n",
    "# Intermediate file - use configured separator\n",
    "controls_df.to_csv('configs/_prep3_controls.csv', index=False, sep=intermediate_sep)\n",
    "print(f\"  Created configs/_prep3_controls.csv ({len(controls_df)} controls, sep='{intermediate_sep}')\")\n",
    "\n",
    "if total_hh_control is None:\n",
    "    raise ValueError(\"Could not find numberOfHouseholds control!\")\n",
    "\n",
    "# Update settings.yaml\n",
    "print(\"\\nUpdating PopSim configuration...\")\n",
    "with open('configs/settings.yaml', 'r') as f:\n",
    "    settings = yaml.safe_load(f)\n",
    "\n",
    "# Geographies from top to bottom: WELT -> STAAT -> ZENSUS1km -> ZENSUS100m\n",
    "settings['geographies'] = ['WELT', 'STAAT', 'ZENSUS1km', 'ZENSUS100m']\n",
    "settings['seed_geography'] = seed_geography\n",
    "settings['total_hh_control'] = total_hh_control\n",
    "\n",
    "# Update input tables\n",
    "idx = next((i for i, t in enumerate(settings['input_table_list']) if t['tablename'] == 'geo_cross_walk'), None)\n",
    "if idx is not None:\n",
    "    settings['input_table_list'] = settings['input_table_list'][:idx + 1]\n",
    "\n",
    "for geo in ['ZENSUS100m', 'ZENSUS1km', 'STAAT', 'WELT']:\n",
    "    settings['input_table_list'].append({\n",
    "        'tablename': f'{geo}_control_data',\n",
    "        'filename': f'control_totals_{geo}.csv'\n",
    "    })\n",
    "\n",
    "# Update output tables\n",
    "if output_everything:\n",
    "    settings['output_tables'] = {'action': 'skip', 'tables': 'geo_cross_walk'}\n",
    "else:\n",
    "    settings['output_tables'] = {\n",
    "        'action': 'include',\n",
    "        'tables': ['expanded_household_ids', \n",
    "                   'summary_ZENSUS100m', 'summary_ZENSUS1km', 'summary_STAAT', 'summary_WELT',\n",
    "                   f'summary_ZENSUS100m_{seed_geography}']\n",
    "    }\n",
    "\n",
    "# Update models\n",
    "settings['models'] = [m for m in settings['models'] if 'sub_balancing' not in m]\n",
    "idx = settings['models'].index('integerize_final_seed_weights')\n",
    "settings['models'].insert(idx + 1, 'sub_balancing.geography=ZENSUS100m')\n",
    "\n",
    "with open('configs/settings.yaml', 'w') as f:\n",
    "    yaml.dump(settings, f, default_flow_style=False)\n",
    "print(\"  Updated configs/settings.yaml\")\n",
    "\n",
    "# Update verification.yaml\n",
    "with open('scripts/verification.yaml', 'r') as f:\n",
    "    verify = yaml.safe_load(f)\n",
    "\n",
    "verify['group_geographies'] = ['WELT', 'STAAT', 'ZENSUS1km', 'ZENSUS100m']\n",
    "verify['seed_cols']['geog'] = seed_geography\n",
    "verify['summaries'] = [\n",
    "    'output/final_summary_ZENSUS100m.csv',\n",
    "    'output/final_summary_ZENSUS1km.csv',\n",
    "    'output/final_summary_STAAT.csv',\n",
    "    'output/final_summary_WELT.csv',\n",
    "    f'output/final_summary_ZENSUS100m_{seed_geography}.csv'\n",
    "]\n",
    "\n",
    "with open('scripts/verification.yaml', 'w') as f:\n",
    "    yaml.dump(verify, f, default_flow_style=False)\n",
    "print(\"  Updated scripts/verification.yaml\")\n",
    "\n",
    "# Update config\n",
    "config['geo_names'] = geo_names\n",
    "config['total_hh_control'] = total_hh_control\n",
    "with open('prep_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "hh_col = 'numberOfHouseholds_ZENSUS100m'\n",
    "total_hh = census_100m[hh_col].sum()\n",
    "print(f\"  Geographic hierarchy: WELT -> STAAT -> ZENSUS1km -> ZENSUS100m\")\n",
    "print(f\"  Geographic units: {len(census_100m):,} (100m), {len(census_1km):,} (1km)\")\n",
    "print(f\"  Total households: {total_hh:,.0f}\")\n",
    "print(f\"  Controls defined: {len(controls_df)}\")\n",
    "print(f\"  Intermediate separator: '{intermediate_sep}'\")\n",
    "print(f\"\\nNext: Edit configs/_prep3_controls.csv to add expressions for controls you want.\")\n",
    "print(\"\\n[Step 2/4] Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Process Controls and MiD Seed Data\n\n1. Edit `configs/_prep3_controls.csv` to add expressions for the controls you want\n2. Run this cell to load MiD data (paths from Configuration) and create final seed files"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\nimport json\nimport re\nimport math\n\nprint(\"[Step 3/4] Processing seed data...\")\nprint(\"=\" * 60)\n\n# Load config\nwith open(\"prep_config.json\", \"r\") as f:\n    config = json.load(f)\n\nfilter_mid = config[\"filter_mid\"]\nkernwo = config[\"kernwo\"]  # List\nregiostar17 = config[\"regiostar17\"]\nintermediate_sep = config.get(\"intermediate_sep\", \",\")\nmid_households_path = config[\"mid_households_path\"]\nmid_persons_path = config[\"mid_persons_path\"]\n\n# Load controls (intermediate file - use configured separator)\nprint(f\"Loading controls template (separator: '{intermediate_sep}')...\")\ncontrols_df = pd.read_csv('configs/_prep3_controls.csv', sep=intermediate_sep)\nprint(f\"  Loaded {len(controls_df)} controls from _prep3_controls.csv\")\n\n# Load seed data (MiD files use semicolon - German standard)\nprint(f\"\\nLoading MiD seed data...\")\nprint(f\"  Households: {mid_households_path}\")\nprint(f\"  Persons: {mid_persons_path}\")\nseed_households = pd.read_csv(mid_households_path, sep=';')\nseed_persons = pd.read_csv(mid_persons_path, sep=';')\n\nprint(f\"  Loaded {len(seed_persons):,} persons, {len(seed_households):,} households\")\n\nif filter_mid:\n    print(f\"\\nApplying MiD filters:\")\n    persons_before = len(seed_persons)\n    households_before = len(seed_households)\n    \n    # Filter by kernwo (day of week) - applies to persons\n    if 'kernwo' in seed_persons.columns:\n        seed_persons = seed_persons[seed_persons['kernwo'].isin(kernwo)]\n        print(f\"  kernwo filter {kernwo}: {persons_before:,} -> {len(seed_persons):,} persons\")\n    \n    # Filter by RegioStaR17\n    if 'RegioStaR17' in seed_persons.columns:\n        seed_persons = seed_persons[seed_persons['RegioStaR17'].isin(regiostar17)]\n    if 'RegioStaR17' in seed_households.columns:\n        seed_households = seed_households[seed_households['RegioStaR17'].isin(regiostar17)]\n    print(f\"  RegioStaR17 filter {regiostar17}: {len(seed_persons):,} persons, {len(seed_households):,} households\")\n\nprint(f\"\\nFinal counts:\")\nprint(f\"  Persons: {len(seed_persons):,}\")\nprint(f\"  Households: {len(seed_households):,}\")\n\n# Essential columns\nessential_cols = {'H_ID', 'H_GEW', 'HP_ID', 'P_ID', 'P_GEW'}\nneeded_cols = essential_cols.copy()\n\n# Extract columns from expressions\npattern = r'\\.(?P<col>[A-Za-z_][A-Za-z0-9_]*)'\nfor expr in controls_df['expression'].dropna():\n    for match in re.finditer(pattern, str(expr)):\n        needed_cols.add(match.group('col'))\n\nprint(f\"\\nColumns needed from expressions: {needed_cols - essential_cols}\")\n\n# Filter to needed columns\np_cols = list(needed_cols.intersection(seed_persons.columns))\nh_cols = list(needed_cols.intersection(seed_households.columns))\n\nseed_persons = seed_persons[p_cols]\nseed_households = seed_households[h_cols]\n\n# Add STAAT geography\nseed_persons['STAAT'] = 1\nseed_households['STAAT'] = 1\n\n# Save (final files - always comma separated for PopSim compatibility)\nseed_persons.to_csv('data/seed_persons.csv', index=False)\nseed_households.to_csv('data/seed_households.csv', index=False)\ncontrols_df.to_csv('configs/controls.csv', index=False)\n\nprint(f\"\\nCreated (all comma-separated for PopSim):\")\nprint(f\"  data/seed_persons.csv ({len(seed_persons)} rows, {len(seed_persons.columns)} cols)\")\nprint(f\"  data/seed_households.csv ({len(seed_households)} rows, {len(seed_households.columns)} cols)\")\nprint(f\"  configs/controls.csv\")\n\nprint(\"\\n[Step 3/4] Complete.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate and Run\n",
    "\n",
    "Validates the setup and provides instructions for running PopSim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "print(\"[Step 4/4] Validating setup...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = []\n",
    "warnings = []\n",
    "\n",
    "# Check files\n",
    "required_files = [\n",
    "    'data/geo_cross_walk.csv',\n",
    "    'data/seed_persons.csv',\n",
    "    'data/seed_households.csv',\n",
    "    'data/control_totals_ZENSUS100m.csv',\n",
    "    'data/control_totals_ZENSUS1km.csv',\n",
    "    'data/control_totals_STAAT.csv',\n",
    "    'data/control_totals_WELT.csv',\n",
    "    'configs/settings.yaml',\n",
    "    'configs/controls.csv',\n",
    "]\n",
    "\n",
    "print(\"\\nChecking files...\")\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        size = os.path.getsize(f)\n",
    "        print(f\"  [OK] {f} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"  [MISSING] {f}\")\n",
    "        errors.append(f\"Missing: {f}\")\n",
    "\n",
    "# Check controls\n",
    "print(\"\\nChecking controls...\")\n",
    "try:\n",
    "    controls = pd.read_csv('configs/controls.csv')\n",
    "    empty = controls['expression'].isna().sum()\n",
    "    if empty > 0:\n",
    "        errors.append(f\"{empty} controls missing expressions\")\n",
    "    else:\n",
    "        print(f\"  {len(controls)} controls, all have expressions\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Error reading controls: {e}\")\n",
    "\n",
    "# Check settings\n",
    "print(\"\\nChecking settings.yaml...\")\n",
    "try:\n",
    "    with open('configs/settings.yaml') as f:\n",
    "        settings = yaml.safe_load(f)\n",
    "    print(f\"  Geographies: {settings.get('geographies')}\")\n",
    "    print(f\"  Total HH control: {settings.get('total_hh_control')}\")\n",
    "except Exception as e:\n",
    "    errors.append(f\"Error reading settings: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "if errors:\n",
    "    print(\"VALIDATION FAILED\")\n",
    "    for e in errors:\n",
    "        print(f\"  - {e}\")\n",
    "else:\n",
    "    print(\"VALIDATION PASSED\")\n",
    "    print(\"\\nReady to run PopSim:\")\n",
    "    print(\"  conda activate popsim\")\n",
    "    print(\"  python run_populationsim.py\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\n[Step 4/4] Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities: Reset\n",
    "\n",
    "Clean up generated files to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def reset(confirm=False):\n",
    "    \"\"\"Delete all generated files.\"\"\"\n",
    "    files = [\n",
    "        'data/geo_cross_walk.csv',\n",
    "        'data/seed_persons.csv',\n",
    "        'data/seed_households.csv',\n",
    "        'data/control_totals_ZENSUS100m.csv',\n",
    "        'data/control_totals_ZENSUS1km.csv',\n",
    "        'data/control_totals_STAAT.csv',\n",
    "        'data/control_totals_WELT.csv',\n",
    "        'data/_census_100m_filtered.parquet',\n",
    "        'data/_census_1km_filtered.parquet',\n",
    "        'configs/controls.csv',\n",
    "        'configs/_prep3_controls.csv',\n",
    "        'prep_config.json',\n",
    "    ]\n",
    "    \n",
    "    existing = [f for f in files if os.path.exists(f)]\n",
    "    \n",
    "    if not existing:\n",
    "        print(\"No files to delete.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Files to delete:\")\n",
    "    for f in existing:\n",
    "        print(f\"  {f}\")\n",
    "    \n",
    "    if not confirm:\n",
    "        print(\"\\nRun reset(confirm=True) to delete.\")\n",
    "        return\n",
    "    \n",
    "    for f in existing:\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted: {f}\")\n",
    "    print(\"\\nReset complete.\")\n",
    "\n",
    "# Show what would be deleted\n",
    "reset(confirm=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}